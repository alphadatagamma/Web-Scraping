{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The world Passport Index\n",
    "\n",
    "#### This is a one of the few projects aimed at analysing countries which is finally supposed to culminate into a Global_living_score. This is one of the first index or factor that we will analyse.\n",
    "\n",
    "The data for this index is present at https://www.henleypassportindex.com/ and https://www.passportindex.org/byIndividualRank.php\n",
    "\n",
    "Techniques Used:\n",
    "\n",
    "1. pdf_parsing using python: Useless as the pdf's lack serious structure which can be parsed into something useful.\n",
    "2. Web scraping using beautiful soup along with requests library\n",
    "    - Using request to get the Url\n",
    "    - Using beautiful soup to parse the html and extract the div tags\n",
    "3. Using regular expressions to get th desired data < Well lets see how that works >\n",
    "\n",
    "It works because websites always have patterns, if not a table. If a table is not present, there will definitely be a pattern in data that can be extracted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pdf file object \n",
    "pdfFileObj = open('index_2020.pdf', 'rb') \n",
    "  \n",
    "# creating a pdf reader object \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "  \n",
    "# printing number of pages in pdf file \n",
    "print(pdfReader.numPages) \n",
    "  \n",
    "# creating a page object \n",
    "pageObj = pdfReader.getPage(0) \n",
    "  \n",
    "# extracting text from page \n",
    "print(pageObj.extractText()) \n",
    "  \n",
    "# closing the pdf file object \n",
    "pdfFileObj.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Requests to get HTML, parsing using Beautiful soup and then extracting using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-238980-24\"></script>\n",
      "<script>\n",
      "  window.dataLayer = window.dataLayer || [];\n",
      "  function gtag(){dataLayer.push(arguments);}\n",
      "  gtag('js', new Date());\n",
      "  gtag('config', 'UA-238980-24',{\n",
      "  'custom_map': {'cID': 'clientId'}});\n",
      "</script>\n",
      "\n",
      "    <meta charset=\"utf-8\">\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, user\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.passportindex.org/byIndividualRank.php'\n",
    "response = get(url)\n",
    "text= response.text\n",
    "print(text[1:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!DOCTYPE html>\n",
      "<html class=\"no_js\" id=\"facebook\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"default\" id=\"meta_referrer\" name=\"referrer\"/>\n",
      "  <script>\n",
      "   window._cstart=+new Date();\n",
      "  </script>\n",
      "  <script>\n",
      "   function envFlush(a){function b(b){for(var c in a)b[c]=a[c]}window.requireLazy?window.requireLazy([\"Env\"],b):(window.Env=window.Env||{},b(window.Env))}envFlush({\"ajaxpipe_token\":\"AXgce04qwki6aivQ\",\"timeslice_heartbeat_config\":{\"pollIntervalMs\":33,\"idleGapThresholdMs\":60,\"ig\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html_soup = BeautifulSoup(text, 'html.parser')\n",
    "text_html = html_soup.prettify()\n",
    "print(text_html[1:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' {\"ae\": \"United Arab Emirates|178|1||1\",\"fi\": \"Finland|171|2||2\",\"lu\": \"Luxembourg|171|3||2\",\"de\": \"Germany|170|4|Image courtesy of Dorik|3\",\"dk\": \"Denmark|170|5||3\",\"nl\": \"Netherlands|170|6|Image courtesy of Dorik|3\",\"at\": \"Austria|170|7|Image courtesy of Martin S.|3\",\"es\": \"Spain|170|8||3\",\"ch\": \"Switzerland|170|9||3\",\"ie\": \"Ireland|170|10||3\",\"kr\": \"South Korea|170|11||3\",\"se\": \"Sweden|169|12|Image courtesy of Vadim Libman|4\",\"fr\": \"France|169|13|Image courtesy of Dorik|4\",\"be\": \"Belgium|169|'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding relevant data from HTML after eyeballing, and then using regex\n",
    "txt1 = re.findall(r'(?<=var obj =).+(?=;var)',text)\n",
    "txt1[0][0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ae United Arab Emirates 178 1 1 fi Finland 171 2 2 lu Luxembourg 171 3 2 de Germany 170 4 Image courtesy of Dorik 3 dk Denmark 170 5 3 nl Netherlands 170 6 Image courtesy of Dorik 3 at Austria 170 7 Image courtesy of Martin S 3 es Spain 170 8 3 ch Switzerland 170 9 3 ie Ireland 170 10 3 kr South Korea 170 11 3 se Sweden 169 12 Image courtesy of Vadim Libman 4 fr France 169 13 Image courtesy of Dorik 4 be Belgium 169 14 4 mt Malta 169 15 4 pt Portugal 169 16 Image courtesy of Dorik 4 no Norway 169 17 4 gr Greece 169 18 4 jp Japan 169 19 4 us United States of America 169 20 4 sg Singapore 168 21 5 it Italy 168 22 Image courtesy of Axel PodestÃ¡ 5 cz Czech Republic 168 23 5 pl Poland 168 24 5 hu Hungary 168 25 Image courtesy of Peter 5 lt Lithuania 168 26 5 sk Slovakia 168 27 Image courtesy of Branislav 5 gb United Kingdom 168 28 5 ca Canada 168 29 5 nz New Zealand 168 30 5 si Slovenia 167 31 6 ee Estonia 167 32 6 lv Latvia 167 33 6 is Iceland 167 34 6 au Australia 167 35 6 cy Cyprus 165 '"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic cleaning 1\n",
    "txt2 = re.sub(r'\\W+',\" \",txt1[0])\n",
    "txt2[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ae United Arab Emirates 178 1 1 fi Finland 171 2 2 lu Luxembourg 171 3 2 de Germany 170 4 3 dk Denmark 170 5 3 nl Netherlands 170 6 3 at Austria 170 7 3 es Spain 170 8 3 ch Switzerland 170 9 3 ie Ireland 170 10 3 kr South Korea 170 11 3 se Sweden 169 12 4 fr France 169 13 4 be Belgium 169 14 4 mt Malta 169 15 4 pt Portugal 169 16 4 no Norway 169 17 4 gr Greece 169 18 4 jp Japan 169 19 4 us United States of America 169 20 4 sg Singapore 168 21 5 it Italy 168 22 5 cz Czech Republic 168 23 5 pl Poland 168 24 5 hu Hungary 168 25 5 lt Lithuania 168 26 5 sk Slovakia 168 27 5 gb United Kingdom 168 28 5 ca Canada 168 29 5 nz New Zealand 168 30 5 si Slovenia 167 31 6 ee Estonia 167 32 6 lv Latvia 167 33 6 is Iceland 167 34 6 au Australia 167 35 6 cy Cyprus 165 36 7 ro Romania 165 37 7 my Malaysia 164 38 8 bg Bulgaria 164 39 8 li Liechtenstein 163 40 9 hr Croatia 163 41 9 mc Monaco 163 42 9 br Brazil 160 43 10 cl Chile 160 44 10 ar Argentina 159 45 11 hk Hong Kong 155 46 12 sm San Marino 153 47'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic cleaning 2: removing the Image courtesy text\n",
    "txt3 = re.sub(r' Image courtesy of [\\D]+',\" \",txt2)\n",
    "txt3[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Country names out\n",
    "\n",
    "country = re.findall(r'(?<= [a-z]{2} )[a-zA-Z ]+(?= \\d)',txt3)\n",
    "len(country)\n",
    "\n",
    "# Gives us 199 countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the ranks and scores\n",
    "ranknscore = re.findall(r'(?<=[a-z] )[\\d ]+(?= [a-z])',txt3)\n",
    "#ranknscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giving structure to dataframe\n",
    "import pandas as pd\n",
    "rank_list = []\n",
    "for i in range(len(country)):\n",
    "    rank_list.append([country[i]]+re.split(\" \",ranknscore[i]))\n",
    "rank_data = pd.DataFrame(rank_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_data.columns = ['Country','Score','Individual_rank','Global_rank']\n",
    "#rank_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Requests to get the JSON object and then its as simple as extracting featuures using the ictionary obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 21786,\n",
       "  'passport_id': 3,\n",
       "  'rank': 1,\n",
       "  'year': 2011,\n",
       "  'score': 173,\n",
       "  'passport': {'id': 3,\n",
       "   'name': 'Finland',\n",
       "   'code': 'FI',\n",
       "   'created_at': '2017-12-27 12:18:38',\n",
       "   'updated_at': '2018-01-07 12:15:51',\n",
       "   'deleted_at': None,\n",
       "   'region_id': 5,\n",
       "   'is_program': 0,\n",
       "   'latitude': '61.92411',\n",
       "   'longitude': '25.748151',\n",
       "   'rank': 4,\n",
       "   'score': 188}}]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"https://www.henleypassportindex.com/fetch?url=rankings%2F2011\")\n",
    "data = response.json()\n",
    "data[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating the process to scrap data for the range of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def extract_rank_year(year):\n",
    "    url = \"https://www.henleypassportindex.com/fetch?url=rankings%2F\"+str(year)\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    temp = []\n",
    "    year_list = []\n",
    "    for i in range(len(data)):\n",
    "        temp = [(data[i]['passport']['name']),(data[i]['year']),(data[i]['score']),(data[i]['rank']),i+1]\n",
    "        #print(temp)\n",
    "        year_list.append(temp)\n",
    "    year_df = pd.DataFrame(year_list)\n",
    "    year_df.columns = [['Country','Year','Score','Global_rank','Individual_rank']]\n",
    "    return(year_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_range(y1,y2):\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(y1,y2+1):\n",
    "        df_temp = extract_rank_year(i)\n",
    "        #print(df_temp)\n",
    "        df = pd.concat([df,df_temp], axis=0, ignore_index = True)\n",
    "    return(df)\n",
    "    \n",
    "#\"https://www.henleypassportindex.com/fetch?url=rankings%2F\"+\"2011\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "Passport_data = extract_year_range(2006,2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "Passport_data.to_csv(\"Passport_power.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
